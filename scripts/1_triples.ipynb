{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149d393c-cf3c-47f4-87a1-e4156c7e4a93",
   "metadata": {},
   "source": [
    "# Extract Triples from Parsed Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ffe59-3bb9-4a4c-85d3-5d03a2902661",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db577f8-4057-46f0-bef2-048ded8effc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from spacy.tokens import Doc, Span, DocBin, Token\n",
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pathvecs.matchers as matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c285ab-fa42-4cd2-bc5f-6bfec1fbfba9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0bd34-103b-43fa-97ce-664afb202683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_triples_df(df):\n",
    "\n",
    "    # Allow neither (src) or (dst) terms to be non-utf8 characters\n",
    "    df = df[df.apply(lambda col: col.str.encode('ascii', errors='ignore').str.decode('utf-8')).all(axis=1)]\n",
    "\n",
    "    # Allow neither (src) or (dst) terms to be space characters\n",
    "    df = df[df.apply(lambda col: ~col.str.isspace()).all(axis=1)]\n",
    "\n",
    "    # Disallow newlines\n",
    "    df = df[df.apply(lambda col: ~col.str.contains('\\n')).all(axis=1)]\n",
    "\n",
    "    # Disallow equals signs (tend to be parts of wikipedia markdown / css elements)\n",
    "    df = df[df.apply(lambda col: ~col.str.contains('=')).all(axis=1)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f0aee-0263-4afa-b15d-57923ada00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_triples_df(df):\n",
    "    \n",
    "    # Normalize spaces as underscores\n",
    "    df['src'] = df['src'].str.replace(' ', '_')\n",
    "    df['dst'] = df['dst'].str.replace(' ', '_')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a901a-a766-4c88-aa4f-756809dafa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_triples(fp):\n",
    "    \n",
    "    IGNORED_DEPS = {'dep', 'det', 'punct', 'pobj', 'ROOT', 'prep', 'cc'}\n",
    "    dep_triples = []\n",
    "    \n",
    "    loaded_doc_bin = DocBin().from_disk(str(fp))\n",
    "    for doc in loaded_doc_bin.get_docs(nlp.vocab):\n",
    "        doc = pronouns_matcher(doc)\n",
    "        doc = triple_matcher(doc)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                \n",
    "                dst = token\n",
    "                if dst._.antecedent is not None:\n",
    "                    dst = dst._.antecedent\n",
    "                \n",
    "                dep = dst.dep_\n",
    "                if dep in IGNORED_DEPS:\n",
    "                    continue\n",
    "                \n",
    "                src = token.head\n",
    "                if src._.antecedent is not None:\n",
    "                    src = src._.antecedent\n",
    "                \n",
    "                # edge cases\n",
    "                if src == dst:\n",
    "                    continue\n",
    "\n",
    "                triple = (src.lemma_.lower(), dep, dst.lemma_.lower())\n",
    "                dep_triples.append(triple)\n",
    "        \n",
    "        for triple in doc._.triples:\n",
    "            \n",
    "            src = triple.src\n",
    "            if src._.antecedent is not None:\n",
    "                src = src._.antecedent\n",
    "    \n",
    "            dst = triple.dst\n",
    "            if dst._.antecedent is not None:\n",
    "                dst = dst._.antecedent\n",
    "\n",
    "            src_text = src.lemma_.lower()\n",
    "            dst_text = dst.lemma_.lower()\n",
    "\n",
    "            # Dont create / add frames that are internal to an entity name\n",
    "            if src == dst:\n",
    "                continue\n",
    "            \n",
    "            if triple.edge.startswith('prep_'):\n",
    "                dep_triples.append((src_text, triple.edge, dst_text))\n",
    "            \n",
    "            else:\n",
    "                # Treat the edge as if it were an active transitive verb\n",
    "                dep_triples.append((triple.edge, 'nsubj', src_text))\n",
    "                dep_triples.append((triple.edge, 'dobj', dst_text))\n",
    "    \n",
    "    df = pd.DataFrame(dep_triples, columns=['src', 'path', 'dst'])\n",
    "    df = clean_triples_df(df)\n",
    "    df = normalize_triples_df(df)\n",
    "\n",
    "    # df['forward_context'] = df['dst'] + '/' + df['dep']\n",
    "    # df['reverse_context'] = df['src'] + '/' + df['dep'] + '-1'\n",
    "\n",
    "    # Save triples\n",
    "    outfp = str(fp).replace('parses', 'triples')\n",
    "    outfp = outfp.replace('.spacy', '.df')\n",
    "    df.to_parquet(outfp)\n",
    "\n",
    "    return [outfp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8226e-e9b5-49e6-91d5-f0d08a5f3106",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a013dc5-6a0f-46a8-86f2-ed1abc6fbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place where the pipeline artifacts are going\n",
    "data_path = Path('../').resolve().joinpath('data')\n",
    "\n",
    "# Name of the input parse folder in data/parses\n",
    "dataset = 'wikipedia_20220101'\n",
    "\n",
    "# Number of DocBin files to process (1000 articles / file)\n",
    "N = 1000\n",
    "\n",
    "# Triple patterns to use\n",
    "triple_patterns = [\n",
    "    'prep',\n",
    "    'intransitive_verb_prep',\n",
    "    'appos_noun_prep',\n",
    "    'be_noun_prep',\n",
    "    'poss_noun_appos',\n",
    "    'poss_noun_prep'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9c06f9-4069-4aca-9e76-104425143d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Model and Pipeline Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a60f84-ec14-46c6-b941-d46458898d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()\n",
    "triple_matcher = nlp.add_pipe('triple_matcher', config={'use_patterns': triple_patterns})\n",
    "pronouns_matcher = nlp.add_pipe('map_relative_pronouns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a26c5-f59d-4614-a11c-224df0cff28f",
   "metadata": {},
   "source": [
    "### Prepare Output Folder\n",
    "**Deletes anything from previous runs!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fe68c-3955-497e-85b9-10bce76e0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = data_path.joinpath('triples', dataset)\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "else:\n",
    "    shutil.rmtree(output_folder)\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92775c-a461-464e-8eed-e1e853a7a358",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818a7b3-5df3-4f35-b7c2-26ae35b71c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_files = list(data_path.joinpath('parses', dataset).rglob('*.spacy'))[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8304da-8d62-4b44-aa20-d7c96d6ac218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=4) as pool:\n",
    "    \n",
    "    num_done = 0\n",
    "    for output in pool.imap_unordered(docs_to_triples, parse_files):\n",
    "        num_done += 1\n",
    "        if num_done % (len(parse_files) // 10) == 0:\n",
    "            print('{}/{}'.format(num_done, len(parse_files)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathvecs-kernel",
   "language": "python",
   "name": "pathvecs-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
