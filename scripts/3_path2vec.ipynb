{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c897c085-353d-483e-9c40-556f116abc2e",
   "metadata": {},
   "source": [
    "# Train a customized word2vec\n",
    "\n",
    "Includes negative sampling rate.</br>\n",
    "Does not include subsampling, this is todo.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effda4fb-7e52-4f50-a193-1b1ccb392527",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5998fe-8304-4bf7-bd4f-7f7c98ac0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pathvecs\n",
    "from pathvecs.pytorch import WordContextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff0f33-80d0-43c3-a610-98fc5daaecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632745d-32e3-4a81-bc0c-fe70c93836e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b814f-e34a-4730-a68c-f6906e1ffe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place where the pipeline artifacts are going. default: {project}/data\n",
    "data_path = Path('../').resolve().joinpath('data')\n",
    "\n",
    "# Name of the input triples folder in data/triples\n",
    "dataset_name = 'wikipedia_20220101'\n",
    "\n",
    "# Batch size to run in the forward pass between weight updates\n",
    "batch_size = 2048\n",
    "\n",
    "# Total passes through the dataset\n",
    "num_epochs = 1\n",
    "\n",
    "# # Words with frequency > ssr are downsampled. 1e-5 used in original paper, 0 for none\n",
    "# subsample_rate = 1e-5\n",
    "\n",
    "# Number of negative examples to pair with each training sample\n",
    "negative_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b1ef7-9776-468f-95d4-fc5b41a93c16",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238abb3-52f0-4a10-8a86-32b150e977a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\" Skip gram with negative sampling \"\"\"\n",
    "\n",
    "    def __init__(self, wvocab, cvocab, emb_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Vocabulary maps\n",
    "        self.w2i = wvocab\n",
    "        self.i2w = {i: w for w, i in wvocab.items()}\n",
    "        \n",
    "        self.c2i = cvocab\n",
    "        self.i2c = {i: c for c, i in cvocab.items()}\n",
    "        \n",
    "        # Model parameters\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "        self.w_embeddings = nn.Embedding(len(wvocab), emb_dim, sparse=True)\n",
    "        self.c_embeddings = nn.Embedding(len(cvocab), emb_dim, sparse=True)\n",
    "    \n",
    "        nn.init.uniform_(self.w_embeddings.weight, -1.0, 1.0)\n",
    "        nn.init.uniform_(self.c_embeddings.weight, -1.0, 1.0)\n",
    "\n",
    "    def forward(self, w_pos, c_pos, c_neg):\n",
    "        \"\"\"\n",
    "        With B = batch_size, N = negative_samples\n",
    "        w_pos: 1 x B\n",
    "        c_pos: 1 x B\n",
    "        c_neg: B x N\n",
    "        \"\"\"\n",
    "\n",
    "        w_emb = self.w_embeddings(w_pos)\n",
    "        c_emb = self.c_embeddings(c_pos)\n",
    "        c_neg_emb = self.c_embeddings(c_neg)\n",
    "\n",
    "        score = torch.sum(torch.mul(w_emb, c_emb), dim=1)\n",
    "        score = F.logsigmoid(score)\n",
    "        \n",
    "        neg_score = torch.bmm(c_neg_emb, w_emb.unsqueeze(2)).squeeze()\n",
    "        neg_score = F.logsigmoid(-neg_score) # \n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        \n",
    "        return torch.sum(score), torch.sum(neg_score)\n",
    "    \n",
    "    def top_w_sims(self, word, k=5):\n",
    "\n",
    "        topk_sims = F.cosine_similarity(\n",
    "            self.w_embeddings.weight[self.w2i[word]],\n",
    "            self.w_embeddings.weight\n",
    "        ).topk(k)\n",
    "        \n",
    "        for wi, sim in zip(topk_sims.indices.data.tolist(), topk_sims.values.data.tolist()):\n",
    "            yield self.i2w[wi], sim\n",
    "\n",
    "\n",
    "def log_sample_neighbors(model, words, k=5):\n",
    "    \n",
    "    data = {}\n",
    "    for i, word in enumerate(words):\n",
    "        si = 's{}'.format(i)\n",
    "        data[word] = []\n",
    "        data[si] =[]\n",
    "        \n",
    "        for other_word, sim in model.top_w_sims(word, k=k):\n",
    "            data[word].append(other_word)\n",
    "            data[si].append('{:2.3f}'.format(sim))\n",
    "\n",
    "    print(tabulate.tabulate(data, headers='keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2fdd5-403b-4f03-bc6f-e0ee22f72d22",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d25462-ef96-4754-a30f-a14c54b44806",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvocab = {}\n",
    "with open(data_path.joinpath('vocab', dataset_name, 'wvocab.txt')) as infile:\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        wvocab[line.strip()] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48e3ac-011b-407e-a0a7-7ab84ffd6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvocab = {}\n",
    "with open(data_path.joinpath('vocab', dataset_name, 'cvocab.txt')) as infile:\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        cvocab[line.strip()] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978384a1-7a28-4352-b97f-0939568f7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-shuffle once up front\n",
    "dataset_fp = data_path.joinpath('pairs', dataset_name, 'pairs.pt')\n",
    "word_context_pairs = torch.load(dataset_fp)\n",
    "word_context_pairs = word_context_pairs[torch.randperm(len(word_context_pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0cc7f-9bbb-4399-b5ec-f62b97b5453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordContextDataset(\n",
    "    pairs_data=word_context_pairs,\n",
    "    negative_samples=negative_samples\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9a1fa-58a6-4c1b-b258-9f1b5a95cd55",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a920ad-d16c-4103-9e3f-b867d62a8dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SkipGramModel(\n",
    "    wvocab=wvocab,\n",
    "    cvocab=cvocab,\n",
    "    emb_dim=128\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    batches = enumerate(iter(dataloader))\n",
    "    for i, (pos_u, pos_v, neg_v) in tqdm(batches, total=len(dataloader)):\n",
    "\n",
    "        pos_u = Variable(pos_u)\n",
    "        pos_v = Variable(pos_v)\n",
    "        neg_v = Variable(neg_v)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pos_score, neg_score = model(pos_u, pos_v, neg_v)\n",
    "\n",
    "        loss = -1 * (pos_score + neg_score).sum() / batch_size\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 2500 == 0:\n",
    "            print('\\nloss:', loss.data.tolist())\n",
    "            print('-'*50)\n",
    "            log_sample_neighbors(model, ['book', 'be_leader_of', 'be_author_of', 'poss_brother_appos', 'lead', 'write', 'move-to'], k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathvecs-kernel",
   "language": "python",
   "name": "pathvecs-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
