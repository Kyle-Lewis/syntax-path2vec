{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ba759c-aeb3-400c-9063-d5344f4726ff",
   "metadata": {},
   "source": [
    "# Precalculate generalized 'word' vocab and context vocab\n",
    "\n",
    "Each triple represents two 'words' and a dependency edge between them.<br/>\n",
    "As per https://aclanthology.org/P14-2050.pdf, take each word and make <br/>\n",
    "a context 'word' from the concatenation of the edge with the other word <br/>\n",
    "of the triple, accounting for direction with a '-1'<br/>\n",
    "\n",
    "For example:<br/>\n",
    "\n",
    "&emsp;*\"Alice threw the ball.\"*<br/><br/>\n",
    "would yield the triples:<br/>\n",
    "\n",
    "&emsp;*(throw)-[nsubj]->(Alice)*<br/>\n",
    "&emsp;*(throw)-[dobj]->(ball)*<br/><br/>\n",
    "From which we want to get the (word, context) pairs:\n",
    "\n",
    "&emsp;throw, Alice/nsubj<br/>\n",
    "&emsp;throw, ball/dobj<br/>\n",
    "&emsp;Alice, throw/nsubj-1<br/>\n",
    "&emsp;ball, throw/dobj-1<br/>\n",
    "\n",
    "Then we can construct a word vocabulary and a context vocabulary constrainted<br/>\n",
    "to the words and contexts that appear at least K times. Because the vocabularies<br/>\n",
    "are disjoint, their sizes will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debace05-7c5c-4663-b4a5-12f81bb3a439",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29105041-f250-461f-8e9d-91fd89f9f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pathvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243ee11-0993-4708-97bb-1863c95c2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO config this\n",
    "data_path = Path(pathvecs.__file__).parents[1].joinpath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57221cb0-c8cf-4ff2-997c-c05a2a895943",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992ce5c-0a5c-4e82-bc5d-1dcb934f0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the input triples folder in data/triples\n",
    "dataset = 'wikipedia_20220101'\n",
    "\n",
    "# Required instances for a word or context to be included in a vocabulary\n",
    "K = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42617a15-2c06-4e8b-9d75-2016dd8b8957",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8458a-b331-4d60-939a-31e51bb5e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate and interleave entries, as each triple corresponds to two pairs\n",
    "# TODO: may need to do this with integer representations for larger datasets\n",
    "wc_pairs = []\n",
    "triples_files = list(data_path.joinpath('triples', dataset).glob('*.df'))\n",
    "\n",
    "for fp in triples_files:\n",
    "    triples = pd.read_parquet(fp, engine='fastparquet')\n",
    "    for src, edge, dst in zip(triples['src'], triples['path'], triples['dst']):\n",
    "    \n",
    "        wc_pairs.append((src, dst + '/' + edge))\n",
    "        wc_pairs.append((dst, src + '/' + edge + '-1'))\n",
    "\n",
    "wc_pairs = pd.DataFrame(wc_pairs, columns=['word', 'context'])\n",
    "wc_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb1e83-fa63-42c1-8a45-27eac8beb7ad",
   "metadata": {},
   "source": [
    "### Prune\n",
    "The arrangement of word,context pairs makes this a big bipartite graph, and </br>\n",
    "removing a word 'node' or a context 'node' from that graph could result in </br>\n",
    "its' neighbors falling below the frequency threshold. So prune iteratively until</br>\n",
    "this doesn't happen.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd00f70-a587-4bbd-8b7e-3ca641a10c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning = True\n",
    "while pruning:\n",
    "\n",
    "    len_before = len(wc_pairs)\n",
    "    \n",
    "    wc_pairs['w_count'] = wc_pairs['word'].map(wc_pairs['word'].value_counts())\n",
    "    wc_pairs['c_count'] = wc_pairs['context'].map(wc_pairs['context'].value_counts())\n",
    "    wc_pairs = wc_pairs.loc[(wc_pairs['w_count'] >= K) & (wc_pairs['c_count'] >= K)]\n",
    "    \n",
    "    num_removed = len_before - len(wc_pairs)\n",
    "    print(\"Removed {:,} pairs below frequency threshold.\".format(num_removed))\n",
    "    pruning = (num_removed != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed10c2-6e0b-43fe-b550-136e8ba9dfdb",
   "metadata": {},
   "source": [
    "### Save Vocab Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e24385-17c1-4b11-8562-122f07ab75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = data_path.joinpath('vocab', dataset)\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436d3db-f11f-4bd1-bb3b-efb8eba8d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will overwrite any existing vocab for the dataset\n",
    "wvocab_file = data_path.joinpath('vocab', dataset, 'wvocab.npy')\n",
    "wc_pairs.sort_values('w_count', ascending=False, inplace=True)\n",
    "wvocab = wc_pairs['word'].unique()\n",
    "np.save(wvocab_file, wvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55819bba-aa0b-406c-a325-c43d3bdb5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will overwrite any existing vocab for the dataset\n",
    "cvocab_file = data_path.joinpath('vocab', dataset, 'cvocab.npy')\n",
    "wc_pairs.sort_values('c_count', ascending=False, inplace=True)\n",
    "cvocab = wc_pairs['context'].unique()\n",
    "np.save(cvocab_file, cvocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (path2vec)",
   "language": "python",
   "name": "path2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
