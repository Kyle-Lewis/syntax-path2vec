{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ba759c-aeb3-400c-9063-d5344f4726ff",
   "metadata": {},
   "source": [
    "# Precalculate generalized 'word' vocab and context vocab\n",
    "\n",
    "Each triple represents two 'words' and a dependency edge between them.<br/>\n",
    "As per https://aclanthology.org/P14-2050.pdf, take each word and make <br/>\n",
    "a context 'word' from the concatenation of the edge with the other word <br/>\n",
    "of the triple, accounting for direction with a '-1'<br/>\n",
    "\n",
    "For example:<br/>\n",
    "\n",
    "&emsp;*\"Alice threw the ball.\"*<br/><br/>\n",
    "would yield the triples:<br/>\n",
    "\n",
    "&emsp;*(throw)-[nsubj]->(Alice)*<br/>\n",
    "&emsp;*(throw)-[dobj]->(ball)*<br/><br/>\n",
    "From which we want to get the (word, context) pairs:\n",
    "\n",
    "&emsp;throw, Alice/nsubj<br/>\n",
    "&emsp;throw, ball/dobj<br/>\n",
    "&emsp;Alice, throw/nsubj-1<br/>\n",
    "&emsp;ball, throw/dobj-1<br/>\n",
    "\n",
    "Then we can construct a word vocabulary and a context vocabulary constrainted<br/>\n",
    "to the words and contexts that appear at least K times. Because the vocabularies<br/>\n",
    "are disjoint, their sizes will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debace05-7c5c-4663-b4a5-12f81bb3a439",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29105041-f250-461f-8e9d-91fd89f9f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57221cb0-c8cf-4ff2-997c-c05a2a895943",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992ce5c-0a5c-4e82-bc5d-1dcb934f0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place where the pipeline artifacts are going. default: {project}/data\n",
    "data_path = Path('../').resolve().joinpath('data')\n",
    "\n",
    "# Name of the input triples folder in data/triples\n",
    "dataset = 'wikipedia_20220101'\n",
    "\n",
    "# Required instances for a word or context to be included in a vocabulary\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42617a15-2c06-4e8b-9d75-2016dd8b8957",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load\n",
    "In order to circumvent memory limits on a single machine, the word-context</br>\n",
    "pair dataset has to be constructed as pairs of mapped integers. This makes</br>\n",
    "it necessary to perform a first pass over the triples files to compute the</br>\n",
    "(un-pruned) vocabularies</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8458a-b331-4d60-939a-31e51bb5e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO map-reduce this\n",
    "wcounts = Counter()\n",
    "ccounts = Counter()\n",
    "triples_files = list(data_path.joinpath('triples', dataset).glob('*.df'))\n",
    "\n",
    "for fp in tqdm(triples_files):\n",
    "    triples = pd.read_parquet(fp, engine='fastparquet')\n",
    "    for src, edge, dst in zip(triples['src'], triples['path'], triples['dst']):\n",
    "        \n",
    "        wcounts[src] += 1\n",
    "        wcounts[dst] += 1\n",
    "        ccounts[dst + '/' + edge] += 1\n",
    "        ccounts[src + '/' + edge + '-1'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bbc82-cac6-4763-9b68-8852c00d9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvocab = {}\n",
    "for wi, (word, count) in enumerate(wcounts.most_common()):\n",
    "    if count < K:\n",
    "        break\n",
    "    wvocab[word] = wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8c881-3338-461c-9e87-8de91d4c3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvocab = {}\n",
    "for ci, (context, count) in enumerate(ccounts.most_common()):\n",
    "    if count < K:\n",
    "        break\n",
    "    cvocab[context] = ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437b864-0145-4a07-be0c-9be3be7f636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_table_size = max(\n",
    "    sum(v for v in wcounts.values() if v >= K),\n",
    "    sum(v for v in ccounts.values() if v >= K)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b9396-d0b2-4281-af5d-834ba8b0312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_table_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1552bde-ff2e-4078-9f01-77bb1c6e5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_pairs = np.zeros((max_table_size, 2), dtype=np.int32)\n",
    "wc_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c1408-0ec5-4301-a751-f4a03bbcc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load pairs where both word and context are possibly in vocab\n",
    "triples_files = list(data_path.joinpath('triples', dataset).glob('*.df'))\n",
    "\n",
    "row = 0\n",
    "for fp in tqdm(triples_files):\n",
    "    triples = pd.read_parquet(fp, engine='fastparquet')\n",
    "    for src, edge, dst in zip(triples['src'], triples['path'], triples['dst']):\n",
    "\n",
    "        src_context = dst + '/' + edge\n",
    "        if src in wvocab and src_context in cvocab:\n",
    "            wi = wvocab[src]\n",
    "            ci = cvocab[src_context]\n",
    "            wc_pairs[row] = [wi, ci]\n",
    "            row += 1\n",
    "\n",
    "        dst_context = src + '/' + edge + '-1'\n",
    "        if dst in wvocab and dst_context in cvocab:\n",
    "            wi = wvocab[dst]\n",
    "            ci = cvocab[dst_context]\n",
    "            wc_pairs[row] = [wi, ci]\n",
    "            row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015d49c-4f73-40c3-a517-01809436f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_pairs = wc_pairs[:row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa25eb-d742-4d0b-bdb0-58f0e648c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_pairs = pd.DataFrame(wc_pairs, columns=['wi', 'ci'])\n",
    "wc_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb1e83-fa63-42c1-8a45-27eac8beb7ad",
   "metadata": {},
   "source": [
    "### Prune\n",
    "The arrangement of word,context pairs makes this a big bipartite graph, and </br>\n",
    "removing a word 'node' or a context 'node' from that graph could result in </br>\n",
    "its' neighbors falling below the frequency threshold. So prune iteratively until</br>\n",
    "this doesn't happen.</br>\n",
    "This treatment of the threshold isn't *explicit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd00f70-a587-4bbd-8b7e-3ca641a10c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning = True\n",
    "# while pruning:\n",
    "\n",
    "#     len_before = len(wc_pairs)\n",
    "    \n",
    "#     wc_pairs['w_count'] = wc_pairs['word'].map(wc_pairs['word'].value_counts())\n",
    "#     wc_pairs['c_count'] = wc_pairs['context'].map(wc_pairs['context'].value_counts())\n",
    "#     wc_pairs = wc_pairs.loc[(wc_pairs['w_count'] >= K) & (wc_pairs['c_count'] >= K)]\n",
    "    \n",
    "#     num_removed = len_before - len(wc_pairs)\n",
    "#     print(\"Removed {:,} pairs below frequency threshold.\".format(num_removed))\n",
    "#     pruning = (num_removed != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed10c2-6e0b-43fe-b550-136e8ba9dfdb",
   "metadata": {},
   "source": [
    "### Save Vocab Files And Mapped Pairs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85438ea6-92e2-4374-bf35-2eb6d131933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will overwrite existing pairs dataset\n",
    "output_folder = data_path.joinpath('pairs', dataset)\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3ff8b-5158-426f-a9ab-0a4f0c7b65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_file = data_path.joinpath('pairs', dataset, 'pairs.pt')\n",
    "pairs = torch.tensor(wc_pairs[['wi', 'ci']].values)\n",
    "torch.save(pairs, pairs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e24385-17c1-4b11-8562-122f07ab75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = data_path.joinpath('vocab', dataset)\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436d3db-f11f-4bd1-bb3b-efb8eba8d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will overwrite any existing vocab for the dataset\n",
    "wvocab_file = data_path.joinpath('vocab', dataset, 'wvocab.txt')\n",
    "with open(wvocab_file, 'w+') as outfile:\n",
    "    for word in wvocab:\n",
    "        outfile.write(word)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55819bba-aa0b-406c-a325-c43d3bdb5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will overwrite any existing vocab for the dataset\n",
    "cvocab_file = data_path.joinpath('vocab', dataset, 'cvocab.txt')\n",
    "with open(cvocab_file, 'w+') as outfile:\n",
    "    for context in cvocab:\n",
    "        outfile.write(context)\n",
    "        outfile.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathvecs-kernel",
   "language": "python",
   "name": "pathvecs-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
